{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJLQoimyVyQ8"
   },
   "source": [
    "### Uncomment and run the following cells if you work on Google Colab :) Don't forget to change your runtime type to GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVV81xc3VyQ9",
    "outputId": "5f91275b-3caf-4588-d4bd-ec11089b1b71"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/kstathou/vector_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0lSFLw3VyRG",
    "outputId": "ae5c260a-7925-4bc6-8aac-b6800017d1e0"
   },
   "outputs": [],
   "source": [
    "# cd vector_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5sOhWL6UVyRQ",
    "outputId": "e6f41a8c-d210-4b33-a1ac-4c3886253920"
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbnscDwgVyRW"
   },
   "source": [
    "### Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v7ftrzzmVyRX"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fU2i4vlCVyRc"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# Used to import data from S3.\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "# Used to create the dense document vectors.\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Used to create and store the Faiss index.\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Used to do vector searches and display the results.\n",
    "from vector_engine.utils import vector_search, id2details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz5YBwU5VyRi"
   },
   "source": [
    "Stored and processed data in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VEANywYAVyRi"
   },
   "outputs": [],
   "source": [
    "# Use pandas to read files from S3 buckets!\n",
    "df = pd.read_csv('s3://vector-search-blog/misinformation_papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "HJXljSbYVyRn",
    "outputId": "1c180fbc-42a4-441a-da47-14f5cc21d826"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>citations</th>\n",
       "      <th>id</th>\n",
       "      <th>is_EN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Corrections Fail: The Persistence of Poli...</td>\n",
       "      <td>An extensive literature addresses citizen igno...</td>\n",
       "      <td>2010</td>\n",
       "      <td>901</td>\n",
       "      <td>2132553681</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A postmodern Pandora's box: anti-vaccination m...</td>\n",
       "      <td>The Internet plays a large role in disseminati...</td>\n",
       "      <td>2010</td>\n",
       "      <td>440</td>\n",
       "      <td>2117485795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spread of (Mis)Information in Social Networks</td>\n",
       "      <td>We provide a model to investigate the tension ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>278</td>\n",
       "      <td>2120015072</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      original_title  ... is_EN\n",
       "0  When Corrections Fail: The Persistence of Poli...  ...     1\n",
       "1  A postmodern Pandora's box: anti-vaccination m...  ...     1\n",
       "2      Spread of (Mis)Information in Social Networks  ...     1\n",
       "\n",
       "[3 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MljadlGpVyRs",
    "outputId": "8f2f5205-772f-4c6b-f445-5dd32350d45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misinformation, disinformation and fake news papers: 8430\n"
     ]
    }
   ],
   "source": [
    "print(f\"Misinformation, disinformation and fake news papers: {df.id.unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyRG1wZLVyRw"
   },
   "source": [
    "The [Sentence Transformers library](https://github.com/UKPLab/sentence-transformers) offers pretrained transformers that produce SOTA sentence embeddings. Checkout this [spreadsheet](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/) with all the available models.\n",
    "\n",
    "In this tutorial, we will use the `distilbert-base-nli-stsb-mean-tokens` model which has the best performance on Semantic Textual Similarity tasks among the DistilBERT versions. Moreover, although it's slightly worse than BERT, it is quite faster thanks to having a smaller size.\n",
    "\n",
    "I use the same model in [Orion's semantic search engine](https://www.orion-search.org/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjF6CrwUVyRx",
    "outputId": "db338335-b032-45f2-db21-8e3f53640b86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245M/245M [00:14<00:00, 16.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the sentence-level DistilBERT\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# Check if GPU is available and use it\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "7a7e927567024c578b33b15000d5e531",
      "da5011a6565e45e2b5d0b37634498648",
      "3e9c24c8488b431fb88a0d045d85b700",
      "7aaf8a423e7f48bbac24d6970ed4dfe9",
      "f9c22552944b4e2dafe1f98170665293",
      "15c5bbc768db41e4bc06c9405539091c",
      "1e1408cddd284bc4a4b5484be0561991",
      "7f1cd9dbb4b742ab8d78e073fb9b0f90"
     ]
    },
    "id": "Y_GS0_CWVyR1",
    "outputId": "4deb0814-1ce9-4ea8-8e50-72992e0ea303"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7e927567024c578b33b15000d5e531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=264.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert abstracts to vectors\n",
    "embeddings = model.encode(df.abstract.to_list(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gE7w-RJbVyR6",
    "outputId": "0451849a-88ef-4aee-be2d-e6ff173782f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorised abstract: (768,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the vectorised abstract: {embeddings[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGV4Je1EVyR_"
   },
   "source": [
    "## Vector similarity search with Faiss\n",
    "[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, even ones that do not fit in RAM. \n",
    "    \n",
    "Faiss is built around the `Index` object which contains, and sometimes preprocesses, the searchable vectors. Faiss has a large collection of [indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes). You can even create [composite indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)). Faiss handles collections of vectors of a fixed dimensionality d, typically a few 10s to 100s.\n",
    "\n",
    "**Note**: Faiss uses only 32-bit floating point matrices. This means that you will have to change the data type of the input before building the index.\n",
    "\n",
    "To learn more about Faiss, you can read their paper on [arXiv](https://arxiv.org/abs/1702.08734).\n",
    "\n",
    "Here, we will the `IndexFlatL2` index:\n",
    "- It's a simple index that performs a brute-force L2 distance search\n",
    "- It scales linearly. It will work fine with our data but you might want to try [faster indexes](https://github.com/facebookresearch/faiss/wiki/Faster-search) if you work will millions of vectors.\n",
    "\n",
    "To create an index with the `misinformation` abstract vectors, we will:\n",
    "1. Change the data type of the abstract vectors to float32.\n",
    "2. Build an index and pass it the dimension of the vectors it will operate on.\n",
    "3. Pass the index to IndexIDMap, an object that enables us to provide a custom list of IDs for the indexed vectors.\n",
    "4. Add the abstract vectors and their ID mapping to the index. In our case, we will map vectors to their paper IDs from MAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kkUDtwHVyR_",
    "outputId": "0f668d02-ef33-4123-ab77-18f72a93ab80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the Faiss index: 8430\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Change data type\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float32\")\n",
    "\n",
    "# Step 2: Instantiate the index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Step 3: Pass the index to IndexIDMap\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "# Step 4: Add vectors and their IDs\n",
    "index.add_with_ids(embeddings, df.id.values)\n",
    "\n",
    "print(f\"Number of vectors in the Faiss index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt1z-433VySE"
   },
   "source": [
    "### Searching the index\n",
    "The index we built will perform a k-nearest-neighbour search. We have to provide the number of neighbours to be returned. \n",
    "\n",
    "Let's query the index with an abstract from our dataset and retrieve the 10 most relevant documents. **The first one must be our query!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "eEeJt7lYVySN",
    "outputId": "771571b3-0200-48b8-f2de-4e8cdbd5ec98"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction number [Formula: see text] for each social media platform. Moreover, we identify information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification.\""
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paper abstract\n",
    "df.iloc[5415, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSuRcH85VySQ",
    "outputId": "cec93a12-e79c-4f0a-fc15-45048a3469aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 distance: [0.0, 1.267284631729126, 62.72160339355469, 63.670326232910156, 64.58393859863281, 67.47344970703125, 67.96402740478516, 69.47564697265625, 72.5633544921875, 74.62230682373047]\n",
      "\n",
      "MAG paper IDs: [3092618151, 3011345566, 3012936764, 3055557295, 3011186656, 3044429417, 3092128270, 3024620668, 3047284882, 3048848247]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the 10 nearest neighbours\n",
    "D, I = index.search(np.array([embeddings[5415]]), k=10)\n",
    "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiO1pa4oVySU",
    "outputId": "24c9d1ea-3951-4b4a-e436-c077f3528d7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The COVID-19 social media infodemic.'],\n",
       " ['The COVID-19 Social Media Infodemic'],\n",
       " ['Understanding the perception of COVID-19 policies by mining a multilanguage Twitter dataset'],\n",
       " ['Covid-19 infodemic reveals new tipping point epidemiology and a revised R formula.'],\n",
       " ['Coronavirus Goes Viral: Quantifying the COVID-19 Misinformation Epidemic on Twitter'],\n",
       " ['Effects of misinformation on COVID-19 individual responses and recommendations for resilience of disastrous consequences of misinformation'],\n",
       " ['Analysis of online misinformation during the peak of the COVID-19 pandemics in Italy'],\n",
       " ['Quantifying COVID-19 Content in the Online Health Opinion War Using Machine Learning'],\n",
       " ['Global Infodemiology of COVID-19: Analysis of Google Web Searches and Instagram Hashtags.'],\n",
       " ['COVID-19-Related Infodemic and Its Impact on Public Health: A Global Social Media Analysis.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the paper titles based on their index\n",
    "id2details(df, I, 'original_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p29pEtGrWUMV",
    "outputId": "b6448614-0a6a-4673-c841-2bfb0340607e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction number [Formula: see text] for each social media platform. Moreover, we identify information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification.\"],\n",
       " [\"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction numbers $R_0$ for each social media platform. Moreover, we characterize information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification.\"],\n",
       " ['The objective of this work is to explore popular discourse about the COVID-19 pandemic and policies implemented to manage it. Using Natural Language Processing, Text Mining, and Network Analysis to analyze corpus of tweets that relate to the COVID-19 pandemic, we identify common responses to the pandemic and how these responses differ across time. Moreover, insights as to how information and misinformation were transmitted via Twitter, starting at the early stages of this pandemic, are presented. Finally, this work introduces a dataset of tweets collected from all over the world, in multiple languages, dating back to January 22nd, when the total cases of reported COVID-19 were below 600 worldwide. The insights presented in this work could help inform decision makers in the face of future pandemics, and the dataset introduced can be used to acquire valuable knowledge to help mitigate the COVID-19 pandemic.'],\n",
       " [\"Many governments have managed to control their COVID-19 outbreak with a simple message: keep the effective '$R$ number' $R<1$ to prevent widespread contagion and flatten the curve. This raises the question whether a similar policy could control dangerous online 'infodemics' of information, misinformation and disinformation. Here we show, using multi-platform data from the COVID-19 infodemic, that its online spreading instead encompasses a different dynamical regime where communities and users within and across independent platforms, sporadically form temporary active links on similar timescales to the viral spreading. This allows material that might have died out, to evolve and even mutate. This has enabled niche networks that were already successfully spreading hate and anti-vaccination material, to rapidly become global super-spreaders of narratives featuring fake COVID-19 treatments, anti-Asian sentiment and conspiracy theories. We derive new tools that incorporate these coupled social-viral dynamics, including an online $R$, to help prevent infodemic spreading at all scales: from spreading across platforms (e.g. Facebook, 4Chan) to spreading within a given subpopulation, or community, or topic. By accounting for similar social and viral timescales, the same mathematical theory also offers a quantitative description of other unconventional infection profiles such as rumors spreading in financial markets and colds spreading in schools.\"],\n",
       " ['Background Since the beginning of the coronavirus disease 2019 (COVID-19) epidemic, misinformation has been spreading\\xa0uninhibited\\xa0over traditional and social media at a rapid pace. We sought to analyze the magnitude of misinformation that is being spread on Twitter\\xa0(Twitter, Inc., San Francisco, CA) regarding the coronavirus epidemic.\\xa0 Materials and methods We conducted a search on Twitter using 14 different trending hashtags and keywords related to the COVID-19 epidemic. We then summarized and assessed individual tweets for misinformation in comparison to verified and peer-reviewed resources. Descriptive statistics were used to compare\\xa0terms and hashtags, and to identify individual tweets and account characteristics. Results The study included 673 tweets. Most tweets were posted by informal individuals/groups (66%), and 129 (19.2%) belonged to verified Twitter accounts. The majority of included tweets contained serious content (91.2%); 548 tweets (81.4%) included genuine information pertaining to the COVID-19 epidemic. Around 70% of the tweets tackled medical/public health information, while the others were pertaining to sociopolitical and financial factors. In total, 153 tweets (24.8%) included misinformation, and 107 (17.4%) included unverifiable information regarding the COVID-19 epidemic. The rate of misinformation was higher among informal individual/group accounts (33.8%, p: <0.001). Tweets from unverified Twitter accounts contained more misinformation (31.0% vs 12.6% for verified accounts, p: <0.001). Tweets from healthcare/public health accounts had the lowest rate of unverifiable information (12.3%, p: 0.04). The number of likes and retweets per tweet was not associated with a difference in either false or unverifiable content. The keyword \"COVID-19\" had the lowest rate of misinformation and unverifiable information, while the keywords \"#2019_ncov\" and \"Corona\" were associated with the highest amount of misinformation and unverifiable content respectively. Conclusions Medical misinformation and unverifiable content pertaining to the global COVID-19 epidemic are being propagated at an alarming rate on social media. We provide an early quantification of the magnitude of misinformation spread and highlight the importance of early interventions in order to curb this phenomenon that endangers public safety at a time when awareness and appropriate preventive actions are paramount.'],\n",
       " ['Abstract The proliferation of misinformation on social media platforms is faster than the spread of Corona Virus Diseases (COVID-19) and it can generate hefty deleterious consequences on health amid a disaster like COVID-19. Drawing upon research on the stimulus-response theory (hypodermic needle theory) and the resilience theory, this study tested a conceptual framework considering general misinformation belief, conspiracy belief, and religious misinformation belief as the stimulus; and credibility evaluations as resilience strategy; and their effects on COVID-19 individual responses. Using a self-administered online survey during the COVID-19 pandemic, the study obtained 483 useable responses and after test, finds that all-inclusive, the propagation of misinformation on social media undermines the COVID-19 individual responses. Particularly, credibility evaluation of misinformation strongly predicts the COVID-19 individual responses with positive influences and religious misinformation beliefs as well as conspiracy beliefs and general misinformation beliefs come next and influence negatively. The findings and general recommendations will help the public, in general, to be cautious about misinformation, and the respective authority of a country, in particular, for initiating proper safety measures about disastrous misinformation to protect the public health from being exploited.'],\n",
       " ['During the Covid-19 pandemics, we also experience another dangerous pandemics based on misinformation. Narratives disconnected from fact-checking on the origin and cure of the disease intertwined with pre-existing political fights. We collect a database on Twitter posts and analyse the topology of the networks of retweeters (users broadcasting again the same elementary piece of information, or tweet) and validate its structure with methods of statistical physics of networks. Furthermore, by using commonly available fact checking software, we assess the reputation of the pieces of news exchanged. By using a combination of theoretical and practical weapons, we are able to track down the flow of misinformation in a snapshot of the Twitter ecosystem. Thanks to the presence of verified users, we can also assign a polarization to the network nodes (users) and see the impact of low-quality information producers and spreaders in the Twitter ecosystem.'],\n",
       " ['A huge amount of potentially dangerous COVID-19 misinformation is appearing online. Here we use machine learning to quantify COVID-19 content among online opponents of establishment health guidance, in particular vaccinations (“anti-vax”). We find that the anti-vax community is developing a less focused debate around COVID-19 than its counterpart, the pro-vaccination (“pro-vax”) community. However, the anti-vax community exhibits a broader range of “flavors” of COVID-19 topics, and hence can appeal to a broader cross-section of individuals seeking COVID-19 guidance online, e.g. individuals wary of a mandatory fast-tracked COVID-19 vaccine or those seeking alternative remedies. Hence the anti-vax community looks better positioned to attract fresh support going forward than the pro-vax community. This is concerning since a widespread lack of adoption of a COVID-19 vaccine will mean the world falls short of providing herd immunity, leaving countries open to future COVID-19 resurgences. We provide a mechanistic model that interprets these results and could help in assessing the likely efficacy of intervention strategies. Our approach is scalable and hence tackles the urgent problem facing social media platforms of having to analyze huge volumes of online health misinformation and disinformation.'],\n",
       " ['BACKGROUND: Although \"infodemiological\" methods have been used in research on coronavirus disease (COVID-19), an examination of the extent of infodemic moniker (misinformation) use on the internet remains limited. OBJECTIVE: The aim of this paper is to investigate internet search behaviors related to COVID-19 and examine the circulation of infodemic monikers through two platforms-Google and Instagram-during the current global pandemic. METHODS: We have defined infodemic moniker as a term, query, hashtag, or phrase that generates or feeds fake news, misinterpretations, or discriminatory phenomena. Using Google Trends and Instagram hashtags, we explored internet search activities and behaviors related to the COVID-19 pandemic from February 20, 2020, to May 6, 2020. We investigated the names used to identify the virus, health and risk perception, life during the lockdown, and information related to the adoption of COVID-19 infodemic monikers. We computed the average peak volume with a 95% CI for the monikers. RESULTS: The top six COVID-19-related terms searched in Google were \"coronavirus,\" \"corona,\" \"COVID,\" \"virus,\" \"corona virus,\" and \"COVID-19.\" Countries with a higher number of COVID-19 cases had a higher number of COVID-19 queries on Google. The monikers \"coronavirus ozone,\" \"coronavirus laboratory,\" \"coronavirus 5G,\" \"coronavirus conspiracy,\" and \"coronavirus bill gates\" were widely circulated on the internet. Searches on \"tips and cures\" for COVID-19 spiked in relation to the US president speculating about a \"miracle cure\" and suggesting an injection of disinfectant to treat the virus. Around two thirds (n=48,700,000, 66.1%) of Instagram users used the hashtags \"COVID-19\" and \"coronavirus\" to disperse virus-related information. CONCLUSIONS: Globally, there is a growing interest in COVID-19, and numerous infodemic monikers continue to circulate on the internet. Based on our findings, we hope to encourage mass media regulators and health organizers to be vigilant and diminish the use and circulation of these infodemic monikers to decrease the spread of misinformation.'],\n",
       " ['Infodemics, often including rumors, stigma, and conspiracy theories, have been common during the COVID-19 pandemic. Monitoring social media data has been identified as the best method for tracking rumors in real time and as a possible way to dispel misinformation and reduce stigma. However, the detection, assessment, and response to rumors, stigma, and conspiracy theories in real time are a challenge. Therefore, we followed and examined COVID-19-related rumors, stigma, and conspiracy theories circulating on online platforms, including fact-checking agency websites, Facebook, Twitter, and online newspapers, and their impacts on public health. Information was extracted between December 31, 2019 and April 5, 2020, and descriptively analyzed. We performed a content analysis of the news articles to compare and contrast data collected from other sources. We identified 2,311 reports of rumors, stigma, and conspiracy theories in 25 languages from 87 countries. Claims were related to illness, transmission and mortality (24%), control measures (21%), treatment and cure (19%), cause of disease including the origin (15%), violence (1%), and miscellaneous (20%). Of the 2,276 reports for which text ratings were available, 1,856 claims were false (82%). Misinformation fueled by rumors, stigma, and conspiracy theories can have potentially serious implications on the individual and community if prioritized over evidence-based guidelines. Health agencies must track misinformation associated with the COVID-19 in real time, and engage local communities and government stakeholders to debunk misinformation.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the paper abstracts based on their index\n",
    "id2details(df, I, 'abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFKvRb4QY-DL"
   },
   "source": [
    "\n",
    "## Putting all together\n",
    "\n",
    "So far, we've built a Faiss index using the misinformation abstract vectors we encoded with a sentence-DistilBERT model. That's helpful but in a real case scenario, we would have to work with unseen data. To query the index with an unseen query and retrieve its most relevant documents, we would have to do the following:\n",
    "\n",
    "1. Encode the query with the same sentence-DistilBERT model we used for the rest of the abstract vectors.\n",
    "2. Change its data type to float32.\n",
    "3. Search the index with the encoded query.\n",
    "\n",
    "Here, we will use the introduction of an article published on [HKS Misinformation Review](https://misinforeview.hks.harvard.edu/article/can-whatsapp-benefit-from-debunked-fact-checked-stories-to-reduce-misinformation/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iDhftkrhX99T"
   },
   "outputs": [],
   "source": [
    "user_query = \"\"\"\n",
    "WhatsApp was alleged to have been widely used to spread misinformation and propaganda \n",
    "during the 2018 elections in Brazil and the 2019 elections in India. Due to the \n",
    "private encrypted nature of the messages on WhatsApp, it is hard to track the dissemination \n",
    "of misinformation at scale. In this work, using public WhatsApp data from Brazil and India, we \n",
    "observe that misinformation has been largely shared on WhatsApp public groups even after they \n",
    "were already fact-checked by popular fact-checking agencies. This represents a significant portion \n",
    "of misinformation spread in both Brazil and India in the groups analyzed. We posit that such \n",
    "misinformation content could be prevented if WhatsApp had a means to flag already fact-checked \n",
    "content. To this end, we propose an architecture that could be implemented by WhatsApp to counter \n",
    "such misinformation. Our proposal respects the current end-to-end encryption architecture on WhatsApp, \n",
    "thus protecting users’ privacy while providing an approach to detect the misinformation that benefits \n",
    "from fact-checking efforts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AFhbGnWZpWN",
    "outputId": "b8a02af0-2f0d-4740-984a-e804405b3e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 distance: [7.384466171264648, 57.3224983215332, 57.3224983215332, 71.48453521728516, 72.06803131103516, 79.13472747802734, 86.0128173828125, 89.91024780273438, 90.76014709472656, 90.76422119140625]\n",
      "\n",
      "MAG paper IDs: [3047438096, 3021927925, 3037966274, 2889959140, 2791045616, 2943077655, 3014380170, 2967434249, 3028584171, 2990343632]\n"
     ]
    }
   ],
   "source": [
    "# For convenience, I've wrapped all steps in the vector_search function.\n",
    "# It takes four arguments: \n",
    "# A query, the sentence-level transformer, the Faiss index and the number of requested results\n",
    "D, I = vector_search([user_query], model, index, num_results=10)\n",
    "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbanjBhBZtWZ",
    "outputId": "9a5d6d97-8983-4253-8095-b7d899e33ac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Can WhatsApp Benefit from Debunked Fact-Checked Stories to Reduce Misinformation?'],\n",
       " ['A Dataset of Fact-Checked Images Shared on WhatsApp During the Brazilian and Indian Elections'],\n",
       " ['A Dataset of Fact-Checked Images Shared on WhatsApp During the Brazilian and Indian Elections'],\n",
       " ['A System for Monitoring Public Political Groups in WhatsApp'],\n",
       " ['Politics of Fake News: How WhatsApp Became a Potent Propaganda Tool in India'],\n",
       " ['Characterizing Attention Cascades in WhatsApp Groups'],\n",
       " ['OS IMPACTOS JURÍDICOS E SOCIAIS DAS FAKE NEWS EM TERRITÓRIO BRASILEIRO'],\n",
       " ['Fake News and Social Media: Indian Perspective'],\n",
       " ['Images and Misinformation in Political Groups: Evidence from WhatsApp in India'],\n",
       " ['Can WhatsApp Counter Misinformation by Limiting Message Forwarding']]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetching the paper titles based on their index\n",
    "id2details(df, I, 'original_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbxFKF-DZxg0",
    "outputId": "d78cdc03-41f6-469d-bf67-8f32001a7415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/vector_engine\n"
     ]
    }
   ],
   "source": [
    "# Define project base directory\n",
    "# Change the index from 1 to 0 if you run this on Google Colab\n",
    "project_dir = Path('notebooks').resolve().parents[1]\n",
    "print(project_dir)\n",
    "\n",
    "# Serialise index and store it as a pickle\n",
    "with open(f\"{project_dir}/models/faiss_index.pickle\", \"wb\") as h:\n",
    "    pickle.dump(faiss.serialize_index(index), h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtSC6oDDjtMA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "001-vector-search.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15c5bbc768db41e4bc06c9405539091c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e1408cddd284bc4a4b5484be0561991": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e9c24c8488b431fb88a0d045d85b700": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Batches: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15c5bbc768db41e4bc06c9405539091c",
      "max": 264,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9c22552944b4e2dafe1f98170665293",
      "value": 264
     }
    },
    "7a7e927567024c578b33b15000d5e531": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e9c24c8488b431fb88a0d045d85b700",
       "IPY_MODEL_7aaf8a423e7f48bbac24d6970ed4dfe9"
      ],
      "layout": "IPY_MODEL_da5011a6565e45e2b5d0b37634498648"
     }
    },
    "7aaf8a423e7f48bbac24d6970ed4dfe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f1cd9dbb4b742ab8d78e073fb9b0f90",
      "placeholder": "​",
      "style": "IPY_MODEL_1e1408cddd284bc4a4b5484be0561991",
      "value": " 264/264 [00:31&lt;00:00,  8.30it/s]"
     }
    },
    "7f1cd9dbb4b742ab8d78e073fb9b0f90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da5011a6565e45e2b5d0b37634498648": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9c22552944b4e2dafe1f98170665293": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
